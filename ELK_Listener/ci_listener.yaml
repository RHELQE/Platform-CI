- job-template:
    name: 'CI-Metrics-Data-Listener'
    description: |
        Managed by Jenkins Job Builder. Do not edit via web
    concurrent: false
    node: 'jslave-platform'
    triggers:
        - ci-trigger:
            jms-selector: "(CI_TYPE = '{index}' AND method = 'build' AND scratch = FALSE) OR (CI_TYPE = 'ci-metricsdata')"
    builders:
        - shell: |
            #!/bin/bash
            kinit -k -t $KEYTAB $PRINCIPAL
            # We use both bash and python so cat the python to its own file
            cat << "EOF" > $WORKSPACE/metricsdatalistener.py
            #!/usr/bin/env python
            import sys
            import os
            import json
            import subprocess
            import re
            import time
            import pdc_client
            import numbers
            import dateutil.parser as dp

            # Print the environment to console log for debugging purposes
            for key in os.environ.keys():
                 print "%s=%s" % (key, os.environ[key])

            # Extract CI_MESSAGE from message
            if "CI_MESSAGE" in os.environ:
                message = os.environ["CI_MESSAGE"]
                parsed_json = json.loads(message)
            else:
                parsed_json = json.loads(sys.stdin)

            # If the index does not yet exist, put it there
            hostindex = '{server}:9200/{index}'
            # This template is for elasticsearch and allows timestamp to be your time field
            # It also adds the .raw fields so you can use the fields to create visualizations in kibana
            indextemplate = '{{"mappings":{{"log":{{"properties":{{"timestamp":{{"type":"date"}}}},"dynamic_templates":[{{"message_field":{{"match":"message","match_mapping_type":"string","mapping":{{"type":"string","index":"analyzed","omit_norms":true}}}}}},{{"string_fields":{{"match":"*","match_mapping_type":"string","mapping":{{"type":"string","index":"analyzed","omit_norms":true,"fielddata":{{"format":"disabled"}},"fields":{{"raw":{{"type":"string","index":"not_analyzed","doc_values":true}}}}}}}}}}]}}}}}}'
            indexes = subprocess.Popen(['curl', '-s', '--ipv4', '{server}:9200/_cat/indices?v'], stdout=subprocess.PIPE).communicate()[0]
            # We use {index} as the index in kibana, which is also what
            # CI Type we are listening for.  If the index is not on the host yet
            # put it there
            if '{index}' not in indexes:
                 subprocess.call(['curl', '--ipv4', '-XPUT', "%s?pretty" % hostindex, '-d', indextemplate])

            # Case 1
            if os.environ["CI_TYPE"] == '{index}':
                 if "scratch" in os.environ:
                    if os.environ["scratch"] == 'true':
                         # This should never happen due to the jms selector
                         print("scratch is true. Exiting..")
                         sys.exit()
                 # We want nvr so we can use it as our elasticsearch ID
                 if "name" in os.environ:
                    n = os.environ["name"]
                 else:
                    n = ''
                 if "version" in os.environ:
                    v = os.environ["version"]
                 else:
                    v = ''
                 if "release" in os.environ:
                    r = os.environ["release"]
                 else:
                    r = ''
      
                 # Create regex so we can check if our logs from brew use the expected time format
                 brew_time_format = re.compile(r'(\d{{4}})-(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[01]) (0[0-9]|1[0-9]|2[0-3]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]).[0-9][0-9][0-9][0-9][0-9][0-9]')
                 # We want to discard packages built against the docs target
                 ignore_docs_target = re.compile(r'(docs)')

                 # Add field that shows package was built in brew for visualizations in kibana
                 parsed_json['Brew Built'] = 'true'
      
                 # Extract fields from message header
                 if "target" in os.environ:
                      parsed_json['target'] = os.environ["target"]
                      if ignore_docs_target.match(parsed_json['target']):
                           print('We dont want doc nvrs. Exiting..')
                           sys.exit()
                 if "scratch" in os.environ:
                      parsed_json['scratch'] = os.environ["scratch"]

                 # Extract fields from message body
                 # For fields I am not yet sure of any restrictions on, 
                 # I just pass the key value pair along to parsed_json
                 # For fields I dont think we need, I just do a pass
                 for key, value in parsed_json['info'].items():
                      if key == 'weight':
                           parsed_json[key] = value
                      elif key == 'parent':
                           parsed_json[key] = value
                      elif key == 'channel_id':
                           if not str(value).isdigit():
                                parsed_json[key] = '-1'
                      elif key == 'request':
                           pass
                      elif key == 'start_time':
                           if not brew_time_format.match(value):
                                parsed_json[key] = 'START_TIME_NOT_VALID'
                      elif key == 'start_ts':
                           if isinstance(value, numbers.Real):
                                parsed_json[key] = value
                           else:
                                parsed_json[key] = 'Invalid Timestamp!'
                      elif key == 'waiting':
                           parsed_json[key] = value
                      elif key == 'awaited':
                           parsed_json[key] = value
                      elif key == 'label':
                           parsed_json[key] = value
                      elif key == 'priority':
                           if not str(value).isdigit():
                                parsed_json[key] = '-1'
                      elif key == 'completion_time':
                           if not brew_time_format.match(value):
                                parsed_json[key] = 'COM_TIME_NOT_VALID'
                                parsed_json['timestamp'] = (int(time.time())*1000)
                           else:
                                parsed_json[key] = value
                      elif key == 'state':
                           if not str(value).isdigit():
                                parsed_json[key] = '-1'
                      elif key == 'create_time':
                           if not brew_time_format.match(value):
                                parsed_json[key] = 'CREATE_TIME_NOT_VALID'
                           else:
                                parsed_json[key] = value
                      elif key == 'create_ts':
                           if isinstance(value, numbers.Real):
                                parsed_json[key] = value
                           else:
                                parsed_json[key] = 'Invalid Timestamp!'
                      elif key == 'owner':
                           parsed_json[key] = value
                      elif key == 'host_id':
                           if not str(value).isdigit():
                                parsed_json[key] = '-1'
                      elif key == 'method':
                           parsed_json[key] = value
                      elif key == 'completion_ts':
                           if isinstance(value, numbers.Real):
                                # We need to create the timestamp field for elasticsearch
                                parsed_json['timestamp'] = ("%.0f" % float(str(value*1000).split("\.")[0]))
                           else:
                                parsed_json['timestamp'] = (int(time.time())*1000)
                                parsed_json[key] = 'Invalid Timestamp!'
                      elif key == 'arch':
                           parsed_json[key] = value
                      elif key == 'id':
                           if not str(value).isdigit():
                                parsed_json[key] = '-1'
                           parsed_json['brew_task_id'] = value
                      elif key == 'result':
                           parsed_json[key] = value
                      else: # Did not expect this key!
                           parsed_json[key] = value
      
      
                 # These fields aren't formatted properly for our output
                 # so we previously added their items individually, now 
                 # we delete them
                 keys_to_remove = ['info', 'rpms']
                 for key in keys_to_remove:
                      del parsed_json[key]
      
                 # We use nvr+brew_task_id as our elasticsearch docid
                 parsed_json['nvr'] = n+'-'+v+'-'+r
                 docid = "%s-%s" % (parsed_json['nvr'], str(parsed_json['brew_task_id']))
                 # It is pivotal the doc has a timestamp for storing it
                 if 'timestamp' not in parsed_json.keys():
                      parsed_json['timestamp'] = (int(time.time())*1000)

                 # rhel-6.9 is not in the PDC yet so we need to use 
                 # rhel-6.8 as a fallback for these packages
                 if 'target' in parsed_json.keys() and parsed_json['target'][:8] == 'rhel-6.9':
                      # Get archs we expect to be tested via these
                      # commands from the PDC
                      pdc = pdc_client.PDCClient("prod")
                      try:
                           ret = pdc['releases']['rhel-6.8']["rpm-mapping"][n]._()
                           expected_archs = ''
                           mymap = 'changeme'
                           for candidate in ret['mapping'].keys():
                                if candidate[:6] == 'Server':
                                     mymap = candidate
                                     break
                           if mymap in ret['mapping']:
                                for key in ret['mapping'][mymap].keys():
                                     if key != 'src':
                                          expected_archs = expected_archs + key + ' '
                                expected_archs = expected_archs[:-1]
                           parsed_json['expected_archs'] = expected_archs
                      except:
                           print('WARNING! PDC Call Failed! Got a rhel-6.9 package so looked it up with rhel-6.8, but didnt find it')
                 else:
                      # Look up package by NVR in the PDC
                      try:
                           PDC_message = subprocess.Popen(['curl', '--insecure', '-XGET', '{pdc_rpm_link}/?name=^%s$&version=%s&release=%s' % (n, v, r)], stdout=subprocess.PIPE).communicate()[0]
                           PDC_parsed = json.loads(PDC_message)
                           expected_archs = ''
                           # If results isnt there, NVR was not in PDC
                           # Can add else and sys.exit later
                           # This adds the archs we expect to test for this nvr
                           if 'results' in PDC_parsed:
                                for id in PDC_parsed['results']:
                                     if id['arch'] != 'src':
                                          expected_archs = expected_archs + id['arch'] + ' '
                                expected_archs = expected_archs[:-1]
                                parsed_json['expected_archs'] = expected_archs
                      except:
                           print('Calling PDC by NVR failed with NVR = %s-%s-%s' % (n, v, r))

            # Case 2 (job metrics)
            elif os.environ["CI_TYPE"] == 'ci-metricsdata':

                 # ISO8601 regex.  We use this for metrics timestamps
                 iso8601 = re.compile(r'(\d{{4}})-(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[01])(|[tT\s])(0[0-9]|1[0-9]|2[0-3]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9])Z')
                 expected_Keys=['component', 'trigger', 'tests', 'job_link_back', 'base_distro', 'compose_id', 'brew_task_id', 'create_time', 'completion_time', 'CI_infra_failure', 'CI_infra_failure_desc', 'job_names', 'CI_tier', 'build_type', 'owner', 'content-length', 'destination', 'expires', 'xunit_links', 'jenkins_build_url']

                 # Get the docid now so we can look up old logs
                 if ('component' in parsed_json.keys()) and ('brew_task_id' in parsed_json.keys()):
                      docid = parsed_json['component']+'-'+parsed_json['brew_task_id']
                 else:
                      print("Could not find component/task_id in metrics data.  Exiting...")
                      sys.exit()

                 old_log_json = subprocess.Popen(['curl', '--ipv4', '-XGET', '{server}:9200/{index}/log/'+docid+'?pretty'], stdout=subprocess.PIPE).communicate()[0]
                 old_log = json.loads(old_log_json)

                 # Check to see if we're getting the keys we expect
                 # If we have unexpected keys, we know to add it and
                 # parse the value for correctness
                 for key, value in parsed_json.items():
                      if key not in expected_Keys:
                           print('WARNING! Unexpected Key! ',key)
                      if key in expected_Keys:
                           expected_Keys.remove(key)

                 # Syntax check the keys that we know how to
                 for key, value in parsed_json.items():
                      if key == 'component':
                           if parsed_json[key].count('-') < 2:
                                parsed_json['nvr'] = 'BAD NVR:'+value[:256]
                           else:
                                parsed_json['nvr'] = value[:256]
                           del parsed_json[key]
                      elif key == 'trigger':
                           if (value != 'manual') and (value != 'git') and (value != 'commit') and (value != 'git push') and (value != 'rhpkg build') and (value != 'brew build'):
                                value = 'null'
                           parsed_json[key] = value
                      elif key == 'tests' and '_source' in old_log.keys():
                           for i, tester in enumerate(parsed_json['tests']):
                                if tester['executor'] == 'beaker':
                                     # Find out how many jobs have
                                     # already been reported by old_log
                                     JOBS_REPORTED = 1
                                     JOBS_LIST = []
                                     if 'job_names' in parsed_json.keys():
                                          JOBS_LIST.append(parsed_json['job_names'])
                                     else:
                                          # Dummy name to be replaced later
                                          JOBS_LIST.append("00000")
                                     for key in old_log['_source'].keys():
                                          if key.startswith( 'job_names' ):
                                               JOBS_REPORTED = JOBS_REPORTED + 1
                                               JOBS_LIST.append(old_log['_source'][key])
                                               JOBS_LIST.sort()
                                     # Have to be careful about not reusing values here, so 
                                     # rewrite dummy job with appropriate number of preexisting jobs
                                     if 'job_names' not in parsed_json.keys():
                                          JOBS_LIST[0] = "DUMMY_" + str(JOBS_REPORTED)
                                     for index in range (0,JOBS_REPORTED):
                                          JOB_TO_ADD=JOBS_LIST[index]
                                          if index == 0:
                                               # This job is the new one, tester is incoming metrics data
                                               parsed_json['beaker_job_1'] = JOB_TO_ADD
                                               if not tester['executed'].isdigit():
                                                    tester['executed'] = '-1'
                                               parsed_json['beaker_tests_exec_1'] = tester['executed']
                                               parsed_json['beaker_arch_1'] = tester['arch']
                                               if not tester['failed'].isdigit():
                                                    tester['failed'] = '-1'
                                               parsed_json['beaker_tests_failed_1'] = tester['failed']
                                          else:
                                               # This job is from old log, so was already stored
                                               myval = 0
                                               # Get index of job from old log
                                               # This only supports up to 9 preexisting jobs
                                               # as key[-1:] only gets the last integer
                                               for key in old_log['_source'].keys():
                                                    if key.startswith( 'beaker_job' ):
                                                         if old_log['_source'][key] == JOB_TO_ADD:
                                                              myval = key[-1:]
                                               if myval == 0:
                                                    print("How did we not find the number associated with this key? %s" % key)
                                               else:
                                                    parsed_json['beaker_job_'+str(index+1)] = old_log['_source']['beaker_job_'+str(myval)]
                                                    parsed_json['beaker_arch_'+str(index+1)] = old_log['_source']['beaker_arch_'+str(myval)]
                                                    parsed_json['beaker_tests_exec_'+str(index+1)] = old_log['_source']['beaker_tests_exec_'+str(myval)]
                                                    parsed_json['beaker_tests_failed_'+str(index+1)] = old_log['_source']['beaker_tests_failed_'+str(myval)]
                                elif tester['executor'] == 'CI-OSP':
                                     # Find out how many jobs have
                                     # already been reported by old_log
                                     JOBS_REPORTED = 1
                                     JOBS_LIST = []
                                     if 'job_names' in parsed_json.keys():
                                          JOBS_LIST.append(parsed_json['job_names'])
                                     else:
                                          # Dummy name to be replaced later
                                          JOBS_LIST.append("00000")
                                     for key in old_log['_source'].keys():
                                          if key.startswith( 'job_names' ):
                                               JOBS_REPORTED = JOBS_REPORTED + 1
                                               JOBS_LIST.append(old_log['_source'][key])
                                               JOBS_LIST.sort()
                                     # Have to be careful about not reusing values here, so 
                                     # rewrite dummy job with appropriate number of preexisting jobs
                                     if 'job_names' not in parsed_json.keys():
                                          JOBS_LIST[0] = "DUMMY_" + str(JOBS_REPORTED)
                                     for index in range (0,JOBS_REPORTED):
                                          JOB_TO_ADD=JOBS_LIST[index]
                                          if index == 0:
                                               # This job is the new one, tester is incoming metrics data
                                               parsed_json['ciosp_job_1'] = JOB_TO_ADD
                                               if not tester['executed'].isdigit():
                                                    tester['executed'] = '-1'
                                               parsed_json['ciosp_tests_exec_1'] = tester['executed']
                                               if not tester['failed'].isdigit():
                                                    tester['failed'] = '-1'
                                               parsed_json['ciosp_tests_failed_1'] = tester['failed']
                                               parsed_json['ciosp_arch_1'] = tester['arch']
                                          else:
                                               # This job is from old log, so was already stored
                                               myval = 0
                                               # Get index of job from old log
                                               # This only supports up to 9 preexisting jobs
                                               # as key[-1:] only gets the last integer
                                               for key in old_log['_source'].keys():
                                                    if key.startswith( 'ciosp_job' ):
                                                         if old_log['_source'][key] == JOB_TO_ADD:
                                                              myval = key[-1:]
                                               if myval == 0:
                                                    print("How did we not find the number associated with this key? %s" % key)
                                               else:
                                                    parsed_json['ciosp_job_'+str(index+1)] = old_log['_source']['ciosp_job_'+str(myval)]
                                                    parsed_json['ciosp_arch_'+str(index+1)] = old_log['_source']['ciosp_arch_'+str(myval)]
                                                    parsed_json['ciosp_tests_exec_'+str(index+1)] = old_log['_source']['ciosp_tests_exec_'+str(myval)]
                                                    parsed_json['ciosp_tests_failed_'+str(index+1)] = old_log['_source']['ciosp_tests_failed_'+str(myval)]
                                else:
                                     print('Unsupported test executor!')
                           del parsed_json['tests'] 
                      elif key == 'tests':
                           for i, tester in enumerate(value):
                                if tester['executor'] == 'beaker':
                                     if 'job_names' in parsed_json.keys():
                                          parsed_json['beaker_job_1'] = parsed_json['job_names']
                                     else:
                                          parsed_json['beaker_job_1'] = 'DUMMY_1'
                                     parsed_json['beaker_arch_1'] = tester['arch']
                                     if not tester['executed'].isdigit():
                                          tester['executed'] = '-1'
                                     parsed_json['beaker_tests_exec_1'] = tester['executed']
                                     if not tester['failed'].isdigit():
                                          tester['failed'] = '-1'
                                     parsed_json['beaker_tests_failed_1'] = tester['failed']
                                elif tester['executor'] == 'CI-OSP':
                                     if 'job_names' in parsed_json.keys():
                                          parsed_json['ciosp_job_1'] = parsed_json['job_names']
                                     else:
                                          parsed_json['beaker_job_1'] = 'DUMMY_1'
                                     parsed_json['ciosp_arch_1'] = tester['arch']
                                     if not tester['executed'].isdigit():
                                          tester['executed'] = '-1'
                                     parsed_json['ciosp_tests_exec_1'] = tester['executed']
                                     if not tester['failed'].isdigit():
                                          tester['failed'] = '-1'
                                     parsed_json['ciosp_tests_failed_1'] = tester['failed']
                                else:
                                     print('Unsupported test executor!')
                           del parsed_json[key]
                      elif key == 'job_link_back':
                           pass
                      elif key == 'CI_tier':
                           if not value.isdigit():
                                parsed_json[key] = '-1'
                      elif key == 'base_distro':
                           parsed_json[key] = value[:256]
                      elif key == 'compose_id':
                           if not value.isdigit():
                                parsed_json[key] = '000000'
                      elif key == 'brew_task_id':
                           if not value.isdigit():
                                parsed_json[key] = '000000'
                      elif key == 'create_time':
                           if not iso8601.match(value):
                                parsed_json[key] = 'CRE_TIME_NOT_ISO8601'
                      elif key == 'completion_time':
                           if not iso8601.match(value):
                                parsed_json[key] = 'COM_TIME_NOT_ISO8601'
                                parsed_json['timestamp'] = (int(time.time())*1000)
                           else:
                           # We end up 9 hours ahead somehow so correct back...
                                parsed_json['timestamp'] = int(dp.parse(parsed_json[key]).strftime('%s'))*1000 - 32400000
                      elif key == 'CI_infra_failure':
                           pass
                      elif key == 'CI_infra_failure_desc':
                           parsed_json[key] = value[:1024]
                      elif key == 'job_names':
                           parsed_json[key] = value[:256]
                      elif key == 'build_type':
                           if (value != 'official') and (value != 'scratch'):
                                value = 'null'
                           parsed_json[key] = value[:64]
                      elif key == 'owner':
                           parsed_json[key] = value[:64]

                 # Add field that shows CI Testing was done for kibana visualizations
                 parsed_json['CI Testing Done'] = 'true'
                 if 'timestamp' not in parsed_json.keys():
                      parsed_json['timestamp'] = (int(time.time())*1000)

                 # If _source not in old_log, there is no old log for this nvr
                 if '_source' in old_log.keys():
                      old_log['_source'].update(parsed_json)
                      parsed_json = old_log['_source']

                 # Find difference between actual and expected archs tested
                 # This can be used to show testing process flaws at some point
                 if 'beaker_arch' in parsed_json.keys() and 'expected_archs' in parsed_json.keys():
                      arch_diff = cmp(sorted(parsed_json['beaker_arch'].split()), sorted(parsed_json['expected_archs'].split()))

            # This should not happen
            else:
                 print('Unsupported CI_TYPE!')

            output = json.dumps(parsed_json)

            # Push the data to elasticsearch
            subprocess.call(['curl', '--ipv4', '-XPUT', hostindex+'/log/'+docid, '-d', output])
            EOF
            chmod 755 $WORKSPACE/metricsdatalistener.py
            python $WORKSPACE/metricsdatalistener.py

## Grouping of jobs
- job-group:
    name: ci-metricsdata-listener
    jobs:
      - 'CI-Metrics-Data-Listener'

## Describes the project
- project:
    name: ci-metrics-listener
    server: LINKTO_ELK_SERVER
    pdc_rpm_link: LINKTO_PDC/api/v1/rpms
    index: brew-taskstatechange
    jobs:
      - ci-metricsdata-listener
